Why do you think that women are not driven away from admitting they are feminists by the fringe?

Tell you what.  Try posting in a few larger subreddits, when it's relevant, that you are a feminists.  You will get responses about how this means you hate men, think sex is rape, and are against equality for men.  The people who *actually* believe these things are a minuscule fringe.  The people who assert that all feminists believe this are significant.  And they are driving women away from the term.

Much like with "liberal" I do not believe that letting the other side define what it means is a good idea.