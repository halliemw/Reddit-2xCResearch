I think its absolutely ridiculous that women are mocked or shamed for breast feeding in public. Breasts were designed for feeding infants.

We have billboards, magazine covers, and advertising with women flashing their breasts in a sexualized manner, often showing more skin, everywhere. Literally everywhere. Breast feeding is a god damn natural. I hate how people try to demean breastfeeding.

When I have babies you will be damned to stop me if I want to breast feed. 