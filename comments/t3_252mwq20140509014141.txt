I didn't realize I was supposed to enjoy sex, too, until I was 19? 20? You don't ever see women enjoying sex on TV, magazines, porn (everyone knows it's just acting). It's very subtle but very pervasive.

In addition, there was a stigma attached to me and my sister having sex with people that didn't exist for my brother. My mom literally told me, once I first started having sex, to "Be careful because you don't want to get a reputation." Like, wut. I should never have called her -_-


In grade school, we were not taught what sex actually was and us women were *forced* to wear a pregnancy suit. You know that scene from Mean Girls where the teacher is like "IF YOU HAVE SEX YOU'LL GET PREGNANT AND DIE"? That 100% sums up the "education" I received. That, the pregnancy suit, and other subtle messages (like not even talking about girls at all) lead to an air that sex is something we simply are there for and that we're not ~~willing~~ eager/enthusiastic participants and god forbid we actually like it.

edit: fixed words to sound less rapey