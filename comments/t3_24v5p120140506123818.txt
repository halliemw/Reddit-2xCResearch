It's sad to see celebrities like Taylor Swift and other from the article who have a lot of influence on young girls say stuff like that.

I think a lot of people just never really learn what feminism actually is. They have a vague idea, and it involves women burning bras and refusing men custody, and they never explore further. And public school social studies/history classes being what they are, they never have to. So it's easier to not upset anybody and just denounce it. 

Which is awful. I mean it's such an important concept that just not ever learning about it shouldn't even be an option. Not everyone has to agree, but you should know for your own sake what you're disagreeing with. It's like if I decided that I'd seen a few headlines about what some evangelical Christian groups do and decided "they're all like that, I don't need to talk to anyone or research anything at all, I'm just going to reject it altogether from here." Which is insane because it's a religion that has roots throughout history and culture that still impact issues today. Ugh. 