I never would have called myself a feminist before last year. I always though feminists were women who didnt shave their armpits, hated men and were possibly lesbians. I even thought feminism was shameful. Now I see that my feelings weren't my own fault but they were caused by the world. I was taught that I should sit down in my box and keep my mouth shut.
Last year I had a class, ironically it was with a professor I absolutely hated. It was Women's History Month and he was telling us about a poetry reading we should attend. He asked this class "who here considers themselves a feminist?". NOBODY raised their hand except for him. The class was about 10 females and 20 males. He was shocked that not even the girls raised their hands. 
He explained feminism in a way that I had never heard it before. He said he was a feminist because he wanted women to be treated as equals to men. He wanted them to have equal pay. He was deeply offended by slut shaming and stereotypes. He wanted women to be able to go out and have a career and not feel guilty about it. 
Once he said all that, I knew I was a feminist. I wanted a family and a husband, but I also wanted a career and I did not want a minivan. Nobody could make me feel bad about that!