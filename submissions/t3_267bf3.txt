I feel like more and more (and especially on the internet) the word feminism isn't perceived to mean what it actually does anymore.  It seems that people are using the word to describe radical, man hating bigoted women.  Why?  How is this less upsetting than saying that all Muslim people are terrorists or that all Christian people affiliate with Westboro Baptist?  It really feels that I have to over justify myself when speaking about the issues of any size a lot of women face for fear of being written off as a (god forbid) feminist.  Anyone else feeling this way as of late?  Its a pretty significant bummer.

Edit: I'm so glad to hear all these well thought out answers, I'm glad we all did this. Also, I'm hearing a lot of "well, sucks but they still call themselves feminists so you're gonna get that".  That's kind of disappointing , isn't that still writing off the entire group before listening to what they are trying to say? Seems like a cop out. But there are many wonderful sides to consider in this, and I thank you all.

Edit again: probably should have used a different word than radical, because I dot fully understand all of the terminology surrounding this issue. Partially why I asked. I supposes I mean people who identify as feminists but don't seem to mind forgetting the equal rights bit. The tumblr femmes I guess. Although, I never knew that was a thing either.